\documentclass[margin,line]{res}

\usepackage[hidelinks]{hyperref}
\usepackage{xspace}
\usepackage{wasysym}
\usepackage{marvosym}

\oddsidemargin -.5in
\evensidemargin -.5in
\textwidth=6.0in
\itemsep=0in
\parsep=0in
\setlength{\pdfpagewidth}{\paperwidth}
\setlength{\pdfpageheight}{\paperheight}

\newenvironment{list1}{
  \begin{list}{\ding{113}}{%
      \setlength{\itemsep}{0in}
      \setlength{\parsep}{0in} \setlength{\parskip}{0in}
      \setlength{\topsep}{0in} \setlength{\partopsep}{0in}
      \setlength{\leftmargin}{0in}}}{\end{list}} % was 0.17in
\newenvironment{list2}{
  \begin{list}{$\bullet$}{%
      \setlength{\itemsep}{0in}
      \setlength{\parsep}{0in} \setlength{\parskip}{0in}
      \setlength{\topsep}{0in} \setlength{\partopsep}{0in}
      \setlength{\leftmargin}{0.2in}}}{\end{list}}

\newenvironment{talkgroup}{\setlength{\parskip}{3pt}\everypar{\hangafter=1\hangindent=1em\relax}\par}{\par\everypar{\hangafter=0\relax}}

\newif\ifpublic
\publicfalse
%\publictrue

\newcommand{\me}{\textbf{R.~J.~Giordano}\xspace}
\newcommand{\mestar}{\textbf{R.~J.~Giordano}$^{\star}$\xspace}
\newcommand{\trevor}{T.~C.~Campbell\xspace}
\newcommand{\trevorstar}{T.~C.~Campbell$^{\star}$\xspace}
\newcommand{\tamara}{T.~Broderick\xspace}
\newcommand{\liam}{L.~Paninski\xspace}
\newcommand{\vhw}{M.~Vilain, \me \& B.~Wellner\xspace}

\ifpublic
	\newcommand{\phonesym}{}
	\newcommand{\myphone}{}
	\newcommand{\email}{\texttt{rgiordan -at- mit.edu}}
\else
	\newcommand{\phonesym}{\phone}
	\newcommand{\myphone}{(805) 501-6754}
    \newcommand{\email}{\url{rgiordan@mit.edu}}
\fi

\newif\ifshowpaperrefs
\showpaperrefstrue

\ifshowpaperrefs
	\newcommand{\paperref}[1]{[\href{#1}{pdf}]}
\else
	\newcommand{\paperref}[1]{}
\fi

\begin{document}

\name{Ryan J. Giordano \vspace*{.1in}}

\begin{resume}
\section{\sc Contact Information}
\vspace{.05in}
\begin{tabular}{@{}p{2in}cp{4in}}
1515 Grant St.	& \Letter &\email  \\
Berkeley, CA, 94703	& \Mundus &\url{rgiordan.github.io} \\
USA		& \phonesym & \myphone \\
\end{tabular}

%\section{\sc Research Interests}
%%complexity of inference,
%large-scale learning,
%Bayesian inference,
%computational genomics.
%%inference algorithms for rich model classes (e.g., Bayesian nonparametric models, probabilistic program),
%%learning theory,
%%interface between learning and inference.


% \section{\sc Academic Experience}

% {\bf Boston University}, Boston, MA USA
% \begin{list1}
% \item[] {Assistant Professor, Department of Mathematics \& Statistics}  \hfill {2020--}
% \item[] {Founding Faculty of Computing \& Data Sciences} \hfill{2020--}
% \end{list1}
%
% {\bf Harvard University, Department of Biostatistics}, Boston, MA USA
% \begin{list1}
% \item[] {Postdoctoral Research Fellow.} Advisor: Jeffrey Miller  \hfill {2018--2019}
% \end{list1}
%{\bf Massachusetts Institute of Technology}, Cambridge, MA USA
%
%{\em Graduate Student} \hfill {\bf September 2012 - February 2018} \\
%{\em Department of EECS and Computer Science and Artificial Intelligence Laboratory}
%%Includes current Ph.D.~research, Ph.D.~and Masters level coursework and research/consulting projects.
%
%{\em Teaching Assistant} \hfill {\bf September 2016 - May 2017} \\
%Held office hours, conducted recitation sessions, graded homework,
%and advised students on class projects for graduate-level machine learning courses (6.862 and 6.867).

% {\bf Microsoft Research New England}, Cambridge, MA USA
% \begin{list1}
% \item[] {Research Intern.} Advisor: Lester Mackey \hfill {2017}
% \end{list1}


\section{\sc Education}

{{\bf Massachusetts Institute of Technology}, Cambridge, MA USA}
\begin{list1}
\item[] {\em Department of EECS, Computer Science \& Artificial Intelligence Lab}
\item[] {Postdoctoral Research Fellow.} Advisor: Tamara Broderick  \hfill {2019--}
\end{list1}


{\bf University of California}, Berkeley, CA USA
\begin{list1}
%\item[] {\em Department of Statistics}
\item[] Ph.D., Statistics. Advisors: Michael I.~Jordan, Jon McAuliffe, Tamara Broderick \hfill {2013--2019}
\end{list1}


{\bf London School of Economics}, London, UK
\begin{list1}
%\item[] {\em Department of Economics}
\item[] MSc. with distinction, Econometrics. \hfill {2007--2009}
\end{list1}

{\bf University of Illinois}, Urbana-Champaign, IL, USA
\begin{list1}
\item[] BA., Mathematics. \hfill {1997--2002}
\item[] BS., Theoretical and Applied Mechanics. \hfill {1997--2002}
\end{list1}


\section{\sc Professional Experience}

{\bf Google Inc.}, Mountain View, CA USA
\begin{list1}
\item[] Senior Engineer, Quantitiative Analysis \hfill {2009-2013}
\end{list1}

{\bf Macquarie Group}, London, UK
\begin{list1}
\item[] Risk Management Intern \hfill {2008}
\end{list1}

{\bf LSE Financial Markets Group}, London, UK
\begin{list1}
\item[] Research Intern \hfill {2007}
\end{list1}

{\bf United States Peace Corps}, Kokshetau, KZ
\begin{list1}
\item[] Successful completion of service as an education volunteer. \hfill {2004-2006}
\end{list1}

{\bf Hewlett-Packard}, Boise, ID
\begin{list1}
\item[] Lifetest Coordinator and Reliability Engineer. \hfill {2002-2004}
\end{list1}



\section{\sc Honors and Awards}

\begin{list1}
%\item[] Founding Faculty of Computing and Data Sciences, Boston University (2020--)
\item[] Hariri Institute Junior Faculty Fellow, Boston University (2020--2023)
\item[] Data Science Faculty Fellow, Boston University (2020--)
\item[] Bayes Comp travel award (2020)
\item[] ISBA@NeurIPS travel award (2016)
\item[] DoD National Defense Science and Engineering Graduate Fellowship (2013--2015)
\item[] NSF Graduate Research Fellowship (2013) {\em (declined for DoD NDSEG)}
\item[] Hertz Fellowship Finalist (2013)
\item[] Summa Cum Laude, Columbia University (2012)
\item[] Phi Beta Kappa (2011)
\item[] Rabi Scholar, Columbia College (2008--2012)
\item[] Intel Science Talent Search Finalist (2008)
\end{list1}



%
%{\bf Columbia University, Columbia College}, New York, NY USA
%
%{\em Undergraduate Researcher} \hfill {\bf June 2011 - May 2012} \\
%Conducted independent research in statistics for neuroscience (advisor: Liam Paninski)
%and Bayesian nonparametric modeling (advisor: Frank Wood).

%\vspace{-.1cm}
%{\em Teaching Assistant} \hfill {\bf June 2011 - May 2012} \\
%Duties included office hours and grading of homework for introductory statistics and data structures courses.

\section{\sc Preprints}

$\bullet$ T.~D.~Nguyen, \me, L.~Masoero, L.~Mackey \& \tamara (2020).
Independent finite approximations for Bayesian nonparametric inference: construction, error bounds, and practical implications.
\emph{arXiv:2009.10780 [stat.ME]}.
\paperref{https://arxiv.org/abs/2009.10780}

$\bullet$ W.\ J.\ Bradshaw, E.\ C.\ Alley, \me, A.\ L.\ Lloyd \& K.\ M.\ Esvelt (2020).
Bidirectional contact tracing dramatically improves COVID-19 control.
\emph{medRxiv 2020.05.06.20093369}.
\paperref{https://doi.org/10.1101/2020.05.06.20093369}

$\bullet$ \me \& Jeffrey W.~Miller (2020).
Robust and Reproducible Model Selection Using Bagged Posteriors.
\emph{arXiv:2007.14845 [stat.ME]}.
\paperref{https://arxiv.org/abs/2007.14845}

$\bullet$ \me \& Jeffrey W.~Miller (2019).
Robust Inference and Model Criticism Using Bagged Posteriors.
\emph{arXiv:1912.07104 [stat.ME]}.
\paperref{https://arxiv.org/abs/1912.07104}

$\bullet$ M.~Shiffman, W.~Stephenson, G.~Schiebinger,  \me,  \trevor, A.~Regev \& \tamara  (2018).
Reconstructing probabilistic trees of cellular differentiation from single-cell RNA-seq data.
\emph{arXiv:1811.11790 [q-bio.QM]}.
\paperref{https://arxiv.org/abs/1811.1179}


%\emph{Under review}.

\section{\sc Publications}

20. A.~K.~Dhaka, A.~Catalina, M.~R.~Andersen, M.~Magnusson, \me, A.~Vehtari (2020).
Robust, Accurate Stochastic Optimization for Variational Inference
In \emph{Proc. of the 34th Annual Conference on Neural Information Processing Systems (NeurIPS)}.
\paperref{https://arxiv.org/abs/2009.00666}

19. \me, M.~Kasprzak, \trevor \& \tamara (2020).
Practical posterior error bounds from variational objectives.
In \emph{Proc.~of the 22nd International Conference on Artificial Intelligence and Statistics (AISTATS)}.
\paperref{https://arxiv.org/abs/1910.04102}

18. B.~Trippe, \me, R.~Agrawal \& \tamara (2019).
LR-GLM: High-Dimensional Bayesian Inference Using Low-Rank Data Approximations.
In \emph{Proc. of the 36th International Conference on Machine Learning (ICML)}.
\paperref{https://arxiv.org/abs/1905.07499}

17.  R.~Agrawal, \me, B.~Trippe \& \tamara (2019).
The kernel interaction trick: fast Bayesian discovery of pairwise interactions in high dimensions.
In \emph{Proc. of the 36th International Conference on Machine Learning (ICML)}.
\paperref{https://arxiv.org/pdf/1905.06501}

16. \me, \trevor, M.~Kasprzak \& \tamara (2019).
Scalable Gaussian process inference with finite-data mean and variance guarantees.
In \emph{Proc.~of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)}.
\paperref{https://arxiv.org/abs/1806.10234}

15. R.~Agrawal, \trevor, \me \& \tamara (2019).
Data-dependent compression of random features for large-scale kernel approximation.
In \emph{Proc.~of the 21st International Conference on Artificial Intelligence and Statistics (AISTATS)}.
\paperref{https://arxiv.org/abs/1810.04249}

14. \trevorstar, \mestar, J.~P.~How \& \tamara (2019).
Truncated Random Measures.
\emph{Bernoulli} 25(2), 1256--1288.
\paperref{http://arxiv.org/abs/1603.00861}

13.  \mestar \& D.~M.~Roy$^{\star}$ (2019).
Sequential Monte Carlo as approximate sampling: bounds, adaptive resampling via $\infty$-ESS, and an application to particle Gibbs.
\emph{Bernoulli} 25(1), 584--622.
\paperref{http://arxiv.org/abs/1503.00966}

12. \mestar \& L.~Mackey$^{\star}$ (2018).
Random feature Stein discrepancies.
In \emph{Proc.\ of the 32nd Annual Conference on Neural Information Processing Systems (NeurIPS)}.
\paperref{https://arxiv.org/abs/1806.07788}

11. \me, R.~P.~Adams \& \tamara (2017).
PASS-GLM: polynomial approximate sufficient statistics for scalable Bayesian GLM inference.
In \emph{Proc.\ of the 31st Annual Conference on Neural Information Processing Systems (NeurIPS)}.
\paperref{http://arxiv.org/abs/1709.09216} \\
\quad$\rhd$ Selected for spotlight presentation (top 22\% of accepted papers)

10. \mestar \& J.~Zou$^{\star}$ (2017).
Quantifying the Accuracy of Approximate Diffusions and Markov Chains.
In \emph{Proc.~of the 19th International Conference on Artificial Intelligence and Statistics (AISTATS)}.
\paperref{http://arxiv.org/abs/1605.06420}

9. \me, \trevor \& \tamara (2016).
Coresets for Scalable Bayesian Logistic Regression.
In \emph{Proc.~of the 30th Annual Conference on Neural Information Processing Systems (NeurIPS)}.
\paperref{http://arxiv.org/abs/1605.06423}

8. \me \& J.~B.~Tenenbaum (2015).
Risk and Regret of Hierarchical Bayesian Learners.
In \emph{Proc.~of the 32nd International Conference on Machine Learning (ICML)}.
\paperref{http://arxiv.org/abs/1505.04984}

7. \mestar, A.~Saeedi$^{\star}$, K.~Narasimhan$^{\star}$ \& V.~K.~Mansinghka (2015).
JUMP-Means: Small-Variance Asymptotics for Markov Jump Processes.
In \emph{Proc.~of the 32nd International Conference on Machine Learning}.
\paperref{http://arxiv.org/abs/1503.00332}

6. \me \& C.~Rudin (2014).
A statistical learning theory framework for supervised pattern discovery.
In {\em Proc.~of SIAM International Conference on Data Mining (SDM)}.
\paperref{http://www.jhhuggins.org/papers/HuRu-SDM-2014.pdf}

5. A.~Pakman, \me, C.~Smith \& \liam (2014).
Fast state-space methods for inferring dendritic synaptic connectivity.
{\em Journal of Computational Neuroscience} 36(3), 415--443.
\paperref{http://www.jhhuggins.org/papers/PHSP-JCNS-2014.pdf}

4. E.~Pnevmatikakis, K.~Rahnama Rad, \me \& \liam (2014).
Fast low-SNR Kalman filtering and forward-backward smoothing via a low-rank perturbative approach.
{\em Journal of Computational and Graphical Statistics} 23(2), 316--339. \paperref{http://www.jhhuggins.org/papers/PRHP-JCGS-2013.pdf}

3. \me \& \liam (2012).
Optimal experimental design for sampling voltage on dendritic trees in the low-SNR regime.
{\em Journal of Computational Neuroscience} 32(2), 347--66.
\paperref{http://www.jhhuggins.org/papers/HuPa-JCNS-2012.pdf}

2. \vhw (2009).
Sources of performance in CRF transfer training: a business name-tagging case study.
In {\em Proc.~of Recent Advances in Natural Language Processing (RANLP)}.
\paperref{http://www.jhhuggins.org/papers/VHW-RANLP-2009.pdf}

1. \vhw (2009).
A simple feature-copying approach to long-distance dependencies.
In {\em Proc.~of the 13th Conference on Computational Natural Language Learning (CONLL)}.
\paperref{http://www.jhhuggins.org/papers/VHW-CONLL-2009.pdf}

$\star$ = contributed equally

\section{\sc Workshop \\ Papers}

3. B.~Trippe, \me \& \tamara (2018).
Fast Bayesian Inference in GLMs with Low Rank Data Approximations.
In \emph{Symposium on Advances in Approximate Bayesian Inference}.

2. \me, L.~Masoero, L.~Mackey \& \tamara (2017).
Generic finite approximations for practical Bayesian nonparametrics.
In \emph{NeurIPS 2017 Workshop on Advances in Approximate Bayesian Inference}.

1. M.~Shiffman, W.~Stephenson, G.~Schiebinger, \trevor, \me, A.~Regev \& \tamara (2017).
Probabilistic reconstruction of cellular differentiation trees from single-cell RNA-seq data.
In \emph{NeurIPS 2017 Workshop on Machine Learning in Computational Biology}.

%\section{\sc Conference Presentations}

\section{\sc Miscellanea}

3. \me, M.~Kasprzak, \trevor \& \tamara (2018).
Practical bounds on the error of Bayesian posterior approximations: A nonasymptotic approach.
\emph{arXiv:1809.09505 [stat.TH]}.
\paperref{https://arxiv.org/abs/1809.09505}

2. \me, A.~Saeedi \& M.~J.~Johnson (2014).
Detailed Derivations of Small-variance Asymptotics for some Hierarchical Bayesian Nonparametric Models.
\emph{arXiv:1501.00052 [stat.ML]}.
\paperref{http://arxiv.org/abs/1501.00052}

1. \me \& F.~Wood (2014).
Infinite structured hidden semi-Markov models.
\emph{arXiv:1407.0044 [stat.ME]}.
\paperref{http://arxiv.org/abs/1407.0044}



\section{\sc Invited Talks}

\textbf{Upcoming} \\[-.5em]

\begin{talkgroup}

ISBA World Meeting, Kunming, China \hfill July 2021

University of Haifa, Haifa, Israel / Virtual \hfill March 2021 \\
Statistics Seminar

SIAM Conference on Computational Science and Engineering (CSE21), Virtual \hfill March 2021 \\
Minisymposium on ``Model error in statistical inverse problems''

Harvard University, Boston, MA \hfill TBD \\
B3D Seminar Series

\end{talkgroup}

\textbf{Previous}

\emph{Using Bagged Posteriors for Robust Inference}

\begin{talkgroup}
Northeastern University, Boston, MA \hfill February 2020 \\
SPIRAL Seminar Series

Oxford University, Oxford, UK \hfill October 2019 \\
Statistics Seminar

Bristol University, Bristol, UK \hfill October 2019 \\
Data Science Seminar \\
Statistics Seminar

Massachusetts Institute of Technology, Cambridge, MA  \hfill November 2019 \\
Doctoral Seminar in Statistics

Broad Institute of MIT and Harvard, Cambridge, MA \hfill December 2019 \\
Models, Inference, and Algorithms

\end{talkgroup}

\emph{Scalable, Reliably Accurate Bayesian Inference via Approximate Likelihoods and Random Features}

\begin{talkgroup}
Google AI, Cambridge, MA \hfill February 2019

Broad Institute of MIT and Harvard, Cambridge, MA \hfill February 2019
% Department of Electrical and Computer Engineering,

Northeastern University, Boston, MA \hfill February 2019

%\emph{Scalable, Reliably Accurate Bayesian Inference via Approximate Likelihoods and Random Features}
%Department of Mathematics \& Statistics,
Boston University, Boston, MA \hfill January 2019

\end{talkgroup}

\emph{Finite-dimensional Approximations of Completely Random Measures}

\begin{talkgroup}
Stochastic Processes and Applications (SPA), Gothenburg, Sweden \hfill June 2018

\end{talkgroup}


\emph{Scaling Bayesian Inference by Constructing Approximating Exponential Families}

\begin{talkgroup}
Boston Bayesian Meetup, Boston, MA \hfill April 2018

Schlumberger Doll Research, Cambridge, MA \hfill April 2018

Raytheon BBN Technologies, Cambridge, MA \hfill February 2018

\end{talkgroup}



\section{\sc Contributed Talks}

%\textbf{Upcoming} \\[-.5em]
%
%\begin{talkgroup}
%
%Bernoulli--IMS One World Symposium \hfill August 2020
%
%\end{talkgroup}



\textbf{Previous}

\emph{Using Bagged Posteriors for Robust Inference}
\begin{talkgroup}
Bayes Comp, Gainesville, FL \hfill January 2020
\end{talkgroup}

\emph{Robustness and scalability of Bayesian nonnegative matrix factorization}
\begin{talkgroup}
Joint Statistical Meeting (JSM), Denver, CO \hfill July 2019
\end{talkgroup}

\emph{Scaling Bayesian Inference by Constructing Approximating Exponential Families}
\begin{talkgroup}
ISBA World Meeting, Edinburgh, Scotland \hfill June 2018
\end{talkgroup}

\emph{Truncated Random Measures}
\begin{talkgroup}
11th Conference on Bayesian Nonparametrics (BNP11), Paris, France \hfill June 2017
\end{talkgroup}

\section{\sc Professional Service}
\textbf{Senior Program Committee}
\begin{list2}
\item {Area Chair}, International Conference on Artificial Intelligence and Statistics (AISTATS), 2021
\item {Area Chair}, Advances in Neural Information Processing Systems (NeurIPS), 2019
\item {Senior Program Committee}, Uncertainty in Artificial Intelligence (UAI), 2019
\end{list2}

\textbf{Journal Reviewing}
\begin{list2}
\item Annals of Statistics
\item Journal of Machine Learning Research
\item PLoS One
\item Technometrics
\end{list2}

\textbf{Conference Reviewing}
\begin{list2}
\item Advances in Neural Information Processing Systems (NeurIPS), 2013--2015, 2016--2018, 2020
\item International Conference on Machine Learning (ICML), 2015--2016, 2020
\item International Conference on Artificial Intelligence and Statistics (AISTATS), 2017--2018
\end{list2}

%\section{\sc Advising}
%
%Ph.D.\ students at MIT I have taken a role in advising
%\begin{list2}
%\item Raj Agrawal \hfill 2017 -
%\item Lorenzo Masoero \hfill 2017 -
%\item Brian Trippe \hfill 2017 -
%\end{list2}

\section{\sc Teaching}

\emph{Boston University}
\begin{list2}
\item Instructor, CAS MA 214 Applied Statistics \hfill Fall 2020
\item Lab Instructor, CAS MA 214 Applied Statistics \hfill Spring 2020
\end{list2}

\emph{Massachusetts Institute of Technology}
\begin{list2}
\item Teaching Assistant, 6.862 Applied Machine Learning (Graduate-level) \hfill 2017
\item Guest Lecturer, 6.438 Fundamentals of Probability \hfill 2016
\item Teaching Assistant, 6.867 Machine Learning (Graduate-level) \hfill 2016
\end{list2}

\emph{Columbia University}
\begin{list2}
\item Teaching Assistant, Data Structures \hfill 2011
\item Guest Lecturer, Statistical Analysis of Neural Data (Graduate-level) \hfill  2011
\end{list2}



%Worked with multiple teams on natural language processing problems such as
%co-reference finding, action recognition in free text, and named entity recognition.
%Projects were implemented in OCaml, Python, and Perl and used techniques such as
%integer linear programming and conditional random fields.


%\section{\sc Computer Skills}
%\begin{list2}
%\item Languages: C++, Java, Python, Perl, Matlab, and OCaml
%\item Algorithms: Markov chain Monte Carlo techniques for Bayesian posterior inference
%\end{list2}

\end{resume}
\end{document}
