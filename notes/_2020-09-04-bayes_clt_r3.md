---
layout: post
title:  The Bayesian Central Limit Theorem "Global Region" Assumptions
date:   2020-09-04 02:00:00 -0800
categories: bayes
---

Edit --- the summary belos isn't bad and is maybe worth keeping but
I was wrong about the counterexample I was going to discuss.

Every proof of the Baysian central limit theorem (BCLT, also known as the
Bernstein-von Mises theorem) divides the proof up into three regions, which Le
Cam [3] calls a strictly local region (in a shrinking neighborhood around the
truth parameter), a semi-local region (in a compact ball around the truth), and
a global region (bounded away from the truth, heading off to infinity).  In the
local region you can use all the usual local tricks like Taylor expanding, and
the rate of convergence is determined by the semi-local region.  The global
region is where the BCLT most differs from the central limit theorem for most
frequentist statistics. The reason for this difference is that the Bayesian
posterior is a functional of the entire likelihood, including parameter values
very far from the truth.  Controlling the behavior in the global region requires
strong assumptions.  (It is worth briefly mentioning that, if this global region
cannot be controlled, then inference using frequentist procedures will be just
as problematic as Bayesian procedures, though the frequentist machinery of
simply computing the distribution under the null will not alert you to this; the
problem with the frequentist procedure will be one of power, not of level.)

In my faltering slog through the BCLT literature, I have found two distinct
types of assumption for dealing with the global region: a strict optimality
condition and a testability condition.  The strict optimality assumption is less
general, but appears to be easier to understand and deal with, and so I found
myself tempted to use the strict optimality condition in my own work. However,
it turns out that the strict optimality fails to apply in an extremely simple
case where the testability condtion does not: that of a univariate normal with
unknown mean and variance.  I found this failure to be a nice illustration of the
sorts of things you have to worry about in the global region of the BCLT, so
here it is.

To state the assumptions and model, we'll need some notation.  Let $$\theta \sim
p(\theta)$$ be the real valued parameter and prior, and $$x_n \sim p(x_n |
\theta)$$ be IID data, with $$x = (x_1, ..., x_N)$$.   We will consider the
frequentist properties of the posterior under some true but unknown
$$\theta_0$$. For some fixed $$\delta > 0$$, the "global region" is $$B_\delta^C = \{
\theta: || \theta - \theta_0 ||_2 \ge \delta \}$$, and we wish to find an
assumption under which $$\lim_{N \rightarrow \infty} p(B_\delta^C | x) = 0$$ in
probability under $$p(x | \theta_0)$$.

The optimality assumption requires that there exists an $$\epsilon >
0$$ such that

$$
\sup_{\theta \in B_\delta^C} \left(
    \frac{1}{N} \sum_{n=1}^N \log p(x_n | \theta_0) -
    \frac{1}{N} \sum_{n=1}^N \log p(x_n | \theta)
\right) \ge \epsilon,
$$

with $$p(x \vert \theta_0)$$ probability approaching 1 as $$N \rightarrow
\infty$$.   An instance of the strict optimality condition can be found in
assumption B3 of Chapter 6.7 of Lehmann [2], who attributes its application in
the BCLT to Bickel.

What does the optimality assumption mean?  Essentially, it assumes that the
empirical log likelihood has a strict optimum in a neighborhood of $$\theta_0$$.
Some stronger conditions that would give rise to this would be if a uniform law
of large numbers (ULLN) held over all of $$\theta$$, and the expected log
likelihood $$\theta \mapsto \mathbb{E}[\log p(x_1 | \theta) | \theta_0 ]$$ has a
global optimum at $$\theta_0$$.  A ULLN over the whole parameter space is
probably more than can be typically asked for, but this assumption has that
flavor.

The testability assumption requires that there exist a sequence of binary-valued
tests, $$\phi_n$$, mapping the data $$x$$ into $$\{0, 1\}$$, such that

$$
\begin{align*}
\lim_{N \rightarrow \infty }\mathbb{E}[ \phi_n(x) | \theta_0 ] =& 0 \\
\inf_{\theta \in B_\delta^C}
\lim_{N \rightarrow \infty }\mathbb{E}[ \phi_n(x) | \theta ] =& 1.
\end{align*}
$$

An instance of the testability assumption can be found in Chapter 10,
theorem 10.1 of van der Vaart [1], who attributes it to Le Cam [3].

What does the testability assumption mean?  Taking $$\phi_n(x) = 1$$ to be a
rejection of the hyothesis that $$\theta = \theta_0$$, it means that you can use
the data to distinguish between the true parameter $$\theta_0$$, and values
bounded away from it in $$B_\delta^C$$, uniformly, with probability approaching
one as $$N \rightarrow \infty$$.

If the optimality assumption holds for all $$\theta_0$$, then  the optimality
assumption implies the testability assumption under standard regularity
conditions. Letting $$\hat\theta$$ be the global maximizer of the log
likelihood, and take the test for the hypotheis $$\theta = \theta_0$$ to be

$$
\phi_n(x) =
\mathbb{I}\left(
  \frac{1}{N} \sum_{n=1}^N \log p(x_n | \hat\theta) -
  \frac{1}{N} \sum_{n=1}^N \log p(x_n | \theta_0) \ge \epsilon
\right).
$$

If parameter generating the data was $$\theta_0$$, then $$\hat\theta \rightarrow
\theta_0$$, and the test converges to $$0$$.  If the parameter generating the
data was some other value, then the test converges to $$1$$ uniformly by the
optimality assumption.

However, a procedure can be testable but not satisfy strict optimality, and
the example of a univariate normal with unknown mean and variance suffices
as a counterexample.



[1] Van der Vaart, Aad W. Asymptotic statistics. Vol. 3. Cambridge university press, 2000.

[2] Lehmann, Erich L., and George Casella. Theory of point estimation. Springer Science & Business Media, 2006.

[3] Le Cam, Lucien Marie. On the Bernstein-von Mises theorem. Department of Statistics, University of California, 1986.

[4] Ghosh, Jayanta K., and R. V. Ramamoorthi. Bayesian nonparametrics. Springer Science & Business Media, 2003.
