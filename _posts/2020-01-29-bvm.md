---
layout: post
title:  "A stroll through the Bayesian central limit theorem."
date:   2020-11-20 02:00:00 -0800
categories: bayes
---

I'd like to provide an intuitive walk-through of a proof of the Bayesian central
limit theorem (BCLT, aka the Bernstein-von Mises theorem).  My approach will be
based principally  on on the proof of Theorem 1.4.2 in the book _Bayesian
Nonparametrics_ by Ghosh and Ramamoorthi, though the elements are to some degree
common to all the proofs of the finite-dimensional BCLT that I am aware of.  In
particular, the division into the three regions I will describe is, as far as I
can am aware, the only game in town for the BCLT, though the precise details of
how the regions are dealt with differ according to the need of the authors.

My goal is not to reproduce the proof in full detail, but rather to sketch the
intuition underlying it.  Consequently, I'll be a little fast and loose about
necessary conditions for standard asymptotic results, and will not always add
qualifying filler like the phrase "under necessary regularity conditions."

Let's assume we have independent and identically distributed (IID) data
$$x=(x_1,\ldots,x_N)$$, a parametric model $$p(x_n | \theta)$$, and a prior
density on the scalar parameter $$p(\theta)$$ which is defined with respect to
the Lebesgue measure.  Idential arguments work with multivariate parameters
where the Taylor series residuals are controlled by the integral form of
the remainder rather than the mean value theorem.  To avoid tedious
notation, we'll just focus on the scalar case.

I'll write log probabilities like so: $$\log p(x_n |
\theta) = \ell(x_n | \theta)$$.  The Bayesian posterior expectation
of some function $$\phi(\theta)$$ is then given by

$$
\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\theta) \exp(\sum_{n=1}^N \ell(x_n | \theta))p(\theta) d\theta}
     {\int \exp(\sum_{n=1}^N \ell(x_n | \theta))p(\theta) d\theta}.
$$

I will focus on attributes of the posterior that can be expressed as such
posterior expectations, which includes both total variation distance (the most
commonly used form of convergence) and series expansions of posterior means
(the use case I am particularly interested in for the Bayesian infinitesimal
jackknife).

### Why is the BCLT harder than the maximum likelihood estimator?

Let us begin by reviewing the proof of the asymptotic normality of the
maximum likelihood estimator (MLE).  This proof is standard, and can
be found, for example, in Chapter 6 of Theory of Point Estimation by Lehmann.

The MLE is defined as follows:

$$
\begin{align*}
\hat\theta :=&
    \mathrm{argmax}_\theta \frac{1}{N}\sum_{n=1}^N \ell(x_n | \theta)\\
\theta_0 :=&
    \mathrm{argmax}_\theta \mathbb{E}_{x_1}[\ell(x_1 | \theta)].
\end{align*}
$$

Here, $$\mathbb{E}_{x_1}$$ denotes expectation over a single datapoint for a
fixed $$\theta$$, so $$\mathbb{E}_{x_1}[\ell(x_1 | \theta)]$$ is a function of
$$\theta$$ alone. One typically has to worry about whether $$\hat\theta$$ and
$$\theta_0$$ are uniquely defined, and establish that $$\hat\theta \rightarrow
\theta_0$$ as $$N$$ grows large.  Let's assume that we have resolved such
concerns.

#### Two tools

For both the MLE and the BCLT, we will rely on two key tools: Taylor series
expansions, and uniform laws of large numbers (ULLNs).  We will introduce these
techniques in the context of the MLE before applying them to the BCLT.

For the Taylor series will be convenient to denote partial derivatives with
subscripts, i.e.,

$$
\begin{align*}
\ell_{(k)}(x_1 \vert \theta) :=
    \left.\frac{\partial^k \ell(x_1 \vert \theta)}{\partial \theta^k}
    \right|_\theta.
\end{align*}
$$

In this notation, we can form the following Taylor series expansion of
the first derivative of the log likelihood around the true parameter:

$$
\begin{align*}
\frac{1}{N}\sum_{n=1}^N \ell_{(1)}(x_n \vert \hat\theta) =
    \frac{1}{N}\sum_{n=1}^N \ell_{(1)}(x_n \vert \theta_0) +
    \frac{1}{N}\sum_{n=1}^N \ell_{(2)}(x_n \vert \theta_0) (\hat\theta - \theta_0) +
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N
        \ell_{(3)}(x_n \vert \tilde{\theta}) (\hat\theta - \theta_0)^2,
\end{align*}
$$

where we have used the mean value theorem to evaluated the Taylor series
residual at some $$\tilde{\theta}$$ where $$|\tilde{\theta} - \theta_0 | <
|\hat\theta - \theta_0 |$$.

By definition of $$\hat\theta$$, $$\frac{1}{N}\sum_{n=1}^N \ell_{(1)}(x_n \vert
\hat\theta) = 0$$, so we can rerrange the Taylor series expansion to
get

$$
\sqrt{N} (\hat\theta - \theta_0) =
\frac{\frac{1}{\sqrt{N}}\sum_{n=1}^N \ell_{(1)}(x_n \vert \theta_0)}
     {\frac{1}{N}\sum_{n=1}^N \ell_{(2)}(x_n \vert \theta_0) +
     \frac{1}{2} \frac{1}{N} \sum_{n=1}^N
         \ell_{(3)}(x_n \vert \tilde{\theta}) (\hat\theta - \theta_0)}.
$$

Note that we haven't precisely solved for $$\hat\theta - \theta_0$$, since
this quantity occurs on the right hand side of the expression as well.

What is the limiting behavior of the right hand of the preceding display?
There are three terms that are easy, and one that will require more effort.
The easiest term is $$\hat\theta - \theta_0$$, which converges to zero.
The next easiest is

$$
\frac{1}{N}\sum_{n=1}^N \ell_{(2)}(x_n \vert \theta_0) \rightarrow
    \mathbb{E}_{x_1}[\ell_{(2)}(x_1 | \theta)] =: -\mathcal{I},
$$

which converges to the negative Fisher information by the ordinary law of
large numbers, since $$\theta_0$$ is fixed, if unknown.  Finally,
since $$\mathbb{E}_{x_1}[\ell_{(1)}(x_n \vert \theta_0] = 0$$ by the
definition of $$\theta_0$$, the ordinary central limit theorem gives

$$
\begin{align*}
\Sigma :=& \mathrm{Cov}_{x_1}(\ell_{(1)}(x_n \vert \theta_0)) \\
\frac{1}{\sqrt{N}}\sum_{n=1}^N \ell_{(1)}(x_n \vert \theta_0)
  \rightsquigarrow& \mathcal{N}(0, \Sigma).
\end{align*}
$$

(In correctly specified models, $$\Sigma = \mathcal{I}^{-1}$$, but I will
keep them separate, in part because I am interested in misspecified models.)

Of course, for any particular $$\theta$$, by the law of large numbers (LLN),

$$
\frac{1}{N}\sum_{n=1}^N \ell(x_n | \theta) \rightarrow
    \mathbb{E}_{x_1}[\ell(x_1 | \theta)].
$$

The posterior involves integrals over $$\theta$$, however, so we'll need a
stronger result---we need the above limit, and limits like it, to hold uniformly
over all $$\theta$$.  We'll talk about uniform LLNs later as needed.

The second key quantity is the limiting Fisher information,

$$
\mathcal{I} :=
    -\left.\frac{\partial^2 \mathbb{E}_x[\ell(x_1, \theta)]}
               {\partial\theta \partial \theta} \right|_{\theta_0}.
$$

We'll assume that we can exchange expectation and differentiation, and that a
uniform LLN holds for the sample derivatives.  For convenience let's also define

$$
\sigma := \mathcal{I}^{-1/2}.
$$

Finally, we'll need to center and scale the parameter $$\theta$$, so we define

$$
\begin{align*}
\tau &:= \sqrt{N} \frac{\theta - \hat\theta}{\sigma}
    \Leftrightarrow \\
\theta &= \frac{\sigma}{\sqrt{N}} \tau + \hat\theta
\end{align*}
$$

Note that $$\hat\theta$$, as the MLE is a function _only_ of the data.  So,
despite having a theta in the symbol, $$\hat\theta$$ is not a Bayesian
parameter. When we condition on the data, every funciton of the data is as good
as a constant.  Similarly, $$\mathcal{I}$$, as a limiting quantity, is simply a
number, as is $$\sqrt{N}$$.  So the maps between $$\theta$$ and $$\tau$$
are simply an affine reparameterization for any fixed $$x$$ and $$N$$.


### The Taylor series

Let us now examine the Taylor series expansion that will be the basis
of our approximation.  By the mean value theorem applied to a second-order
expansion around some value $$\tilde\theta$$,

$$
\begin{align*}
\sum_{n=1}^N \ell(x_n | \theta) - \sum_{n=1}^N \ell(x_n | \tilde\theta) =
    &\sum_{n=1}^N \ell_{(1)}(x_n | \tilde\theta) (\theta - \tilde\theta) + \\
    &\frac{1}{2}
        \sum_{n=1}^N \ell_{(2)}(x_n | \tilde\theta)
            (\theta - \tilde\theta)^2 + \\
    &\frac{1}{6}
        \sum_{n=1}^N \ell_{(3)}(x_n | \bar\theta)
            (\theta - \tilde\theta)^3,
\end{align*}
$$

for some $$\bar\theta$$ such that
$$|\bar\theta - \tilde\theta| \le |\bar\theta - \theta|$$.  Each of these
terms is still $$O(N)$$, so no LLN can be applied.  But by substititing
subsitituting $$\tilde\tau = \sqrt{N} (\theta - \tilde\theta)$$
(note that the derivatives do not change --- we are only substituting the
arguments), we get

$$
\begin{align*}
\sum_{n=1}^N \ell(x_n | \theta) - \sum_{n=1}^N \ell(x_n | \tilde\theta) =
    &\frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \tilde\theta)
        \tilde\tau + \\
    &\frac{1}{2} \frac{1}{N}
        \sum_{n=1}^N \ell_{(2)}(x_n | \tilde\theta) \tilde\tau^2 + \\
    &\frac{1}{6} \frac{1}{N^{3/2}}
        \sum_{n=1}^N \ell_{(3)}(x_n | \bar\theta)
            \bar\tau^3,
\end{align*}
$$

where $$\bar\tau = \sqrt{N} (\theta - \bar\theta)$$.  After this substitution,
we see that, for a particular $$\theta$$, though both individual terms on the
left hand side are $$O(N)$$, their difference is $$O(1)$$, since each term on
the right hand side converges.

Now, by an ordinary central limit theorem (CLT), we expect the first term,
$$\frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \tilde\theta)$$, to converge
to a non-degenerate random variable.  So centering at a generic $$\tilde\theta$$
leaves residual randomness.  However by taking $$\tilde\theta = \hat\theta$$,
this term disappears because, by the first-order condition, $$\hat\theta$$ is
the value that sets the gradient of $$\frac{1}{N} \ell_{(1)}(x_n | \theta)$$ to
zero.  This is the reason we center $$\tau$$ at $$\hat\theta$$ and not some
other value.  Plugging in, and recalling the additional re-scaling of $$\tau$$
by $$\sigma$$ gives

$$
\begin{align*}
\sum_{n=1}^N \ell(x_n | \theta) - \sum_{n=1}^N \ell(x_n | \hat\theta) =
    &\frac{1}{2} \sigma^2  \frac{1}{N}
        \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta) \tau^2 +
    \frac{1}{N^{3/2}} \sigma^3 \frac{1}{6}
        \sum_{n=1}^N \ell_{(3)}(x_n | \bar\theta)
            \bar\tau^3.
\end{align*}
$$

Finally, note that the coefficient of the $$\tau^2$$ term,
$$\sigma^2  \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)$$, is a
function of the data alone and, by definition of $$\sigma$$, converges to
$$-1$$, leaving

$$
\begin{align*}
\sum_{n=1}^N \ell(x_n | \theta) - \sum_{n=1}^N \ell(x_n | \hat\theta) =
    -\frac{1}{2} \tau^2 +
    \frac{1}{N^{3/2}} \sigma^3 \frac{1}{6}
        \sum_{n=1}^N \ell_{(3)}(x_n | \bar\theta)
            \bar\tau^3 + ??? \tau^2
\end{align*}
$$


### Uniform laws of large numbers

Uniform LLNs are a complicated subject in
their own right, and I recommend Chapter 18 of _Asymptotic Statistics_ by van
der Vaardt for an introduction.  The key term is that we require the functions
$$\theta \mapsto \ell(x_1, \theta)$$ (or whatever function for which we want
a uniform LLN) to be in a "Glivenko Cantelli" class of functions, meaning
that the map is not too expressive.



### Old text


Under certain circumstances, as $$N \rightarrow \infty$$, the posterior
approaches a particular normal distribution.  Let's take a minute to think about
why this might be.  The quantity inside the exponent, $$\sum_{n=1}^N \ell(x_n |
\theta)$$, in general may be quite complicated as a function of $$\theta$$.
However, for any fixed $$\theta$$, it grows linearly in $$N$$.  After
exponentiating, the highest parts of the function $$\theta \mapsto \sum_{n=1}^N
\ell(x_n | \theta)$$ will be _much_ higher than any surrounding areas.  After
normalizing by the integral in the denominator of $$p(\theta | x)$$, the
posterior will be negligibly small in any diminishing region except around the
maximum.

Consqeuently, the posterior _concentrates_---all its mass goes to a small
region. Now, if the function $$\theta \mapsto \sum_{n=1}^N \ell(x_n | \theta)$$
is smooth, then its behavior in a small region will be well-approximated by a
quadratic function of $$\theta$$, and log quadratic densities are Normal
densities.

Now, it does not suffice to show that quadaratic approximation is good merely
for any fixed $$\theta$$, since posterior quantities typically depend on the
global behavior of $$p(\theta | x)$$ for multiple $$\theta$$.  For example, a
posterior expectations is an integral over $$p(\theta | x)$$.  So we need to
show that integrating against the posterior is asymptotically equivalent to
integrating against a log quadratic density, after appropriate re-scaling.
This, then, will be the whole game---to show that we can Taylor expand the map
$$\theta \mapsto \sum_{n=1}^N \ell(x_n | \theta)$$ to second order, and show
that integrals against the second order approximation are asymptotically the
same as integrals against the full posterior.
