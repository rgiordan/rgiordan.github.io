---
layout: post
title:  "A stroll through the Bayesian central limit theorem.  Part 2: The actual BCLT."
date:   2020-12-04 02:00:00 -0800
categories: bayes
---

In the previous post, I introduced some notation and concepts that I'll
now carry forward into an actual sketch of how to prove the BCLT.

Recall that, for the BCLT we need to analyze quantites of the following form:

$$
\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\theta) \exp(\sum_{n=1}^N \ell(x_n | \theta)) \pi(\theta) d\theta}
     {\int \exp(\sum_{n=1}^N \ell(x_n | \theta)) \pi(\theta) d\theta}.
$$

(We may let $$\phi$$ depend implicitly on $$N$$, e.g., to evaluate
functions of the form $$\phi(\sqrt{N}(\theta - \hat\theta)), for example.)

And recall that, to prove asymptotic normality of the MLE, we studied
the asymptotic behavior of the Taylor expansion of the
random log likelihood function,

$$
\theta \mapsto \frac{1}{N} \sum_{n=1}^N \ell(x_n \vert \theta).
$$

The factor of $$1/N$$ in the MLE is a stark difference with the Bayesian
case, as the posterior involves the random function

$$
\theta \mapsto \sum_{n=1}^N \ell(x_n | \theta),
$$

which diverges as $$N \rightarrow \infty$$!  Before even beginning, we must
re-write our expectation in a form involving something that looks like sample
averages, not a divergent sum.

# Re-write a posterior expectation as a random function that doesn't diverge.

To do so, we'll first form a Taylor series expansion.  For the moment, we'll
expand around a generic $$\bar\theta$$.

$$
\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta)(\theta - \bar\theta) +
    \frac{1}{2}\sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)(\theta - \bar\theta)^2 +
    \frac{1}{6}\sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)(\theta - \bar\theta)^3.
$$

At first glance, each term in the Taylor series still contains $$N$$ terms,
so it might not seem like we've gotten much.  However, we expect to be
able to choose $$\bar\theta$$ so that $$|\theta - \bar\theta| = O(1 / \sqrt{N})$$,
so that higher powers of $$\theta - \bar\theta$$ will decrease asymptotically.

So let's try a parameterization that inflates $$\theta$$ at the rate
$$\sqrt{N}$$.  Specifically, for any $$\bar\theta$$ depending only on the data,
we can define the Bayesian parameter

$$
\tau := \sqrt{N} (\theta - \bar\theta)
\quad\quad \Leftrightarrow \quad \quad
\theta = \frac{\tau}{\sqrt{N}} + \bar\theta.
$$

Plugging in, we have

$$
\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta) \tau +
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)\tau^2 +
    \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3.
$$

Note that in this new formula, the derivatives are still with respect to
$$\theta$$ --- we've simply plugged $$\tau$$ into the previous expression.

Now we're getting somewhere, because the quadratic term looks like what we
expect from a normal distribution, and the cubic term is a sample average
times an additional power of $$1/\sqrt{N}$$, and so goes to zero.

Two terms need to be dealt with.  First, the term $$\frac{1}{\sqrt{N}}
\sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta) \tau$$ is $$O_p(1)$$ by an ordinary
central limit theorem.  Fortunately, we can get rid of this term by simply
evaluating at $$\bar\theta = \hat\theta$$, since then $$\sum_{n=1}^N
\ell_{(1)}(x_n | \bar\theta)$$ is identically zero by the first-order condition
defining $$\hat\theta$$. And, second, we observe that $$\sum_{n=1}^N \ell(x_n |
\bar\theta)$$ does not depend on $$\theta$$, and so cancels in the numerator and
denominator of $$\mathbb{E}[\phi(\theta) | x]$$.

Before we plug into $$\mathbb{E}[\phi(\theta) | x]$$, we need to deal with the
effect of the change of variables on the prior.  By the ordinary rules of
density transformation, $$\pi(\theta) d\theta$$ = $$\frac{1}{\sqrt{N} }\pi(\tau /
\sqrt{N} + \hat\theta) d\tau$$.  Again, the factor of $$\frac{1}{\sqrt{N} }$$
will cancel in the numerator and denominator.  Putting all together, we get

$$
\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\tau / \sqrt{N} + \hat\theta) \exp\left(
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)\tau^2 +
    \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3
\right)
    \pi(\tau / \sqrt{N} + \hat\theta) d\tau}
 {\int \exp\left(
     \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)\tau^2 +
     \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3
  \right)
  \pi(\tau / \sqrt{N} + \hat\theta) d\tau}.
$$

We can now study $$\mathbb{E}[\phi(\theta) | x]$$ by studying integrals
of the following form, depending on some generic function $$\psi(\theta)$$:

$$
I(\psi) :=
    \int \psi(\tau / \sqrt{N} + \hat\theta) \exp\left(
        \frac{1}{2}
            \left( \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta) \right) \tau^2 +
        \frac{1}{6} \frac{1}{\sqrt{N}}
            \left( \frac{1}{N} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta) \right) \tau^3
    \right)
        \pi(\tau / \sqrt{N} + \hat\theta) d\tau,
$$

since $$\mathbb{E}[\phi(\theta) | x] = I(\phi) / I(1)$$.  Importantly,
$$I(\psi)$$ is an integral over quantities that will not diverge as $$N
\rightarrow \infty$$, and we have accomplished the goal of this first section.


# Dependence on the whole domain.

As we mentioned in the last post, a key difficulty with Bayesian CLTs compared
to the behavior of the MLE is that the integral $$I(\psi)$$ still depends on the
entire domain of $$\theta$$ (or, equivalently, $$\tau$$) through the integral.
Let's now think about what sorts of problems we might have with $$I(\psi)$$ for
$$\theta$$ (or, equivalently, $$\tau$$) in different regions of the real line.

What might we hope for?  When $$N$$ is large, then, by a ULLN applied to
$$\ell_{(2)}(x_n | \theta)$$,

$$
\begin{align*}
\frac{1}{N} \ell_{(2)}(x_n | \hat\theta) \approx -\mathcal{I}.
\end{align*}
$$

Can we hope to control $$\frac{1}{N} \sum_{n=1}^N \ell_{(3)}(x_n |
\tilde\theta)$$ by a ULLN?  Recall from the Taylor series that, for a given
$$\theta$$ (or, equivalently, a given $$\tau$$), all we know is that

$$
\vert \tilde\theta - \hat\theta \vert \le
    \Big\vert \theta - \hat\theta \Big\vert \le
    \Big\vert \frac{\tau}{\sqrt{N}} \Big\vert.
$$

And recall from the previous post that it is not reasonable to expect a ULLN to
hold over unbounded domains.  Consequently, we can only apply a ULLN to apply to
$$\frac{1}{N} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)$$ for $$\theta$$ in a
fixed ball around $$\hat\theta$$, or, equivalently, $$\tau$$ in a domain that
grows no faster than $$\sqrt{N}$$.  But, of course, for any $$N$$, the integral
$$I(\psi)$$ also depends on values of $$\tau$$ that are arbitrarily large.
For such large $$\tau$$ we must control $$I(\psi)$$ some other way.


# The big region

Consequently, we first consider the region

$$
R_{big} = \left\{ \theta: |\theta - \hat\theta| > \delta_1 \right\},
$$

for some $$\delta_1$$.  Equivalently, we can write

$$
R_{big} = \left\{ \tau: |\tau| > \sqrt{N} \delta_1 \right\}.
$$

Let $$I_{big}(\psi)$$ denote the integral restricted to the region
$$R_{big}$$.

Within $$R_{big}$$ our Taylor series is of no use, since we cannot
control the $$\frac{1}{N} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)$$ term.
However, we can plug the original function back into the Taylor series to
write

$$
\begin{align*}
I_{big}(\psi) ={}&
    \int_{\tau \in R_{big}}
    \psi(\tau / \sqrt{N} + \hat\theta) \exp\left(
        \sum_{n=1}^N \ell(x_n | \tau / \sqrt{N} + \hat\theta) -
        \sum_{n=1}^N \ell(x_n | \hat\theta)
    \right)
        \pi(\tau / \sqrt{N} + \hat\theta) d\tau \\
={}&
\int_{\tau \in R_{big}}
\psi(\tau / \sqrt{N} + \hat\theta) \exp\left(
    \sum_{n=1}^N \ell(x_n | \theta) -
    \sum_{n=1}^N \ell(x_n | \hat\theta)
\right)
    \pi(\tau / \sqrt{N} + \hat\theta) d\tau.
\end{align*}
$$

One particularly simple way to control this integral is to assume that
$$\hat\theta$$ is a strict optimum of the data log likelihood, in
the sense that, for any $$\delta_1$$, there exists some $$\epsilon$$
such that

$$
\sup_{\theta \in R_{big}}
\frac{1}{N} \sum_{n=1}^N \ell(x_n | \theta) -
\frac{1}{N} \sum_{n=1}^N \ell(x_n | \hat\theta) < -\epsilon.
$$

This condition will fail, for example, if the data log likelihood has multiple
modes that are equally large.  In such a case we surely wouldn't expect a BCLT
to hold.

This is a strange condition, since it's an assumption on the data, rather than
on any population quantity.  One would rather say that $$\mathbb{E}[ \ell(x_1 |
\theta) ]$$ has a strict optimum (where the expectation is over $$x_1$$), and
that $$\frac{1}{N} \sum_{n=1}^N \ell(x_n | \theta)$$ approach $$\mathbb{E}[
\ell(x_1 | \theta) ]$$ sufficiently uniformly for all $$\theta$$ such that
$$\frac{1}{N} \sum_{n=1}^N \ell(x_n | \theta)$$ inherits this property.  But of
course it is unreasonable to expect a ULLN to hold for all $$\theta \in
R_{big}$$.

There is a more elegant but arguably less transparent solution to this
region due to Le Cam --- see chapter 20 of van Der Vaart's Asymptotic
Statistics for example.

When we have strict optimality,

$$
\begin{align*}
\vert I_{big}(\psi)\vert \le{}&
\int_{\tau \in R_{big}}
\vert\psi(\tau / \sqrt{N} + \hat\theta)\vert \exp\left(
    \sup_{\theta \in R_{big}}
        \sum_{n=1}^N \ell(x_n \vert \theta) -
        \sum_{n=1}^N \ell(x_n \vert \hat\theta)
\right)
    \pi(\tau / \sqrt{N} + \hat\theta) d\tau
\\ \le{}&
\int_{\tau \in R_{big}}
\vert\psi(\tau / \sqrt{N} + \hat\theta)\vert \exp\left(
    N \sup_{\theta \in R_{big}}
        \frac{1}{N} \sum_{n=1}^N \ell(x_n \vert \theta) -
        \frac{1}{N} \sum_{n=1}^N \ell(x_n \vert \hat\theta)
\right)
    \pi(\tau / \sqrt{N} + \hat\theta) d\tau
\\ \le{}&
\exp\left(- N \epsilon\right)
\int_{\theta \in R_{big}} \vert\psi(\theta)\vert \pi(\theta) d\theta.
\end{align*}
$$

Consequently, as long as $$\int \vert\psi(\theta)\vert \pi(\theta) d\theta$$
is finite --- i.e., $$\vert\psi(\theta)\vert$$ has finite prior
expectation --- then the contribution to $$I(\psi)$$ from region
$$R_{big}$$ decays exponentially quickly in $$N$$!

It's worth mentioning before we go on that $$R_{big}$$ is the only region
in which we need to worry about the prior $$\pi(\theta)$$ being proper,
or the function $$\psi(\theta)$$ being unbounded, since from now on
we will be limiting ourselves to the ball $$\mathbb{R} \setminus R_{big}$$.


# The small region

Next, the easy region.  Suppose we have a shrinking ball centered at
$$\hat\theta$$ called region 1, or $$R_1$$.  For example, we might take
some real number $\delta_1 > 0$ and define

$$
R_1 = \left\{ \theta: |\theta - \hat\theta| < \delta_1 / \log N \right\}.
$$


We can equivalently write

$$
R_1 = \left\{ \tau: |\tau| < \frac{\sqrt{N}}{\log N} \delta_1  \right\}.
$$

By letting $$R_1$$ shrink at a rate $$1 / \log N$$, we have defined a shrinkig
region in $$\theta$$, but a growing region in $$\tau$$.
