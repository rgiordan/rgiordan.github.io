{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I build autograd's neural net example in `paragami`.  \n",
    "\n",
    "In `paragami`, the code is more verbose, but (I would argue), significantly more readable and less-error prone.  It turns out that `paragami` is measurably faster that `autograd.misc.flatten`, though I'm not sure how well the speed improvement generalizes.\n",
    "\n",
    "https://github.com/HIPS/autograd/blob/master/examples/neural_net_regression.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import autograd\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.random as npr\n",
    "from autograd import grad\n",
    "from autograd.misc import flatten\n",
    "import autograd.scipy.stats.norm as norm\n",
    "from autograd.misc.optimizers import adam\n",
    "\n",
    "import paragami\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_toy_dataset(n_data=80, noise_std=0.1):\n",
    "    rs = npr.RandomState(0)\n",
    "    inputs  = np.concatenate([np.linspace(0, 3, num=n_data/2),\n",
    "                              np.linspace(6, 8, num=n_data/2)])\n",
    "    targets = np.cos(inputs) + rs.randn(n_data) * noise_std\n",
    "    inputs = (inputs - 4.0) / 2.0\n",
    "    inputs  = inputs[:, np.newaxis]\n",
    "    targets = targets[:, np.newaxis] / 2.0\n",
    "    return inputs, targets\n",
    "\n",
    "inputs, targets = build_toy_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's copy the example autograd implementation.  Each layer has a set of weight and bias parameters.  The example uses `autograd.misc.flatten` to represent this parameter set as a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_random_params(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    return [(rs.randn(insize, outsize) * scale,   # weight matrix\n",
    "             rs.randn(outsize) * scale)           # bias vector\n",
    "            for insize, outsize in zip(layer_sizes[:-1], layer_sizes[1:])]\n",
    "\n",
    "def nn_predict(params, inputs, nonlinearity=np.tanh):\n",
    "    for W, b in params:\n",
    "        outputs = np.dot(inputs, W) + b\n",
    "        inputs = nonlinearity(outputs)\n",
    "    return outputs\n",
    "\n",
    "# Note that this may silently do a nonsense thing if you later make params\n",
    "# contain additional parameters that are not biases or weights.  In this\n",
    "# toy model it's probably fine, but in more complicated settings this\n",
    "# is bad modeling practice in my opinion.\n",
    "def log_gaussian(params, scale):\n",
    "    flat_params, _ = flatten(params)\n",
    "    return np.sum(norm.logpdf(flat_params, 0, scale))\n",
    "\n",
    "def logprob(weights, inputs, targets, noise_scale=0.1):\n",
    "    predictions = nn_predict(weights, inputs)\n",
    "    return np.sum(norm.logpdf(predictions, targets, noise_scale))\n",
    "\n",
    "def objective(weights, t):\n",
    "    return -logprob(weights, inputs, targets)\\\n",
    "           -log_gaussian(weights, weight_prior_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement the same thing using `paragami`.  Rather than store the parameters as a list of tuples, we store it as a dictionary of dictionaries, represented with a corresponding pattern.\n",
    "\n",
    "We'll also represent the prior and model parameters with a dictionary.  This will allow us to perform sensivitiy analysis with `vittles` (which is beyond the scope of the present notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Each layer has its own pattern, with `w` for weights and `b` for biases.\n",
    "def get_layer_pattern(input_size, output_size):\n",
    "    layer_pattern = paragami.PatternDict()\n",
    "    layer_pattern['w'] = paragami.NumericArrayPattern((input_size, output_size))\n",
    "    layer_pattern['b'] = paragami.NumericVectorPattern(output_size)\n",
    "    return layer_pattern\n",
    "\n",
    "# The pattern for the whole network is a dictionary of the layer patterns.\n",
    "def get_nn_pattern(layer_sizes):\n",
    "    pattern = paragami.PatternDict(free_default=True)\n",
    "    for l in range(len(layer_sizes) - 1):\n",
    "        insize = layer_sizes[l]\n",
    "        outsize = layer_sizes[l + 1]\n",
    "        pattern[str(l)] = get_layer_pattern(insize, outsize)\n",
    "    return pattern\n",
    "\n",
    "prior_pattern = paragami.PatternDict(free_default=False)\n",
    "prior_pattern['w_scale'] = paragami.NumericScalarPattern(lb=0)\n",
    "prior_pattern['b_scale'] = paragami.NumericScalarPattern(lb=0)\n",
    "prior_pattern['noise_scale'] = paragami.NumericScalarPattern(lb=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_random_params_paragami(scale, layer_sizes, rs=npr.RandomState(0)):\n",
    "    \"\"\"Build a list of (weights, biases) tuples, one for each layer.\"\"\"\n",
    "    params = dict()\n",
    "    for l in range(len(layer_sizes) - 1):\n",
    "        insize = layer_sizes[l]\n",
    "        outsize = layer_sizes[l + 1]\n",
    "        params[str(l)] = dict()\n",
    "        params[str(l)]['w'] = rs.randn(insize, outsize) * scale\n",
    "        params[str(l)]['b'] = rs.randn(outsize) * scale\n",
    "    return params\n",
    "\n",
    "def nn_predict_paragami(params, inputs, nonlinearity=np.tanh):\n",
    "    num_layers = len(params)\n",
    "    for l in range(num_layers):\n",
    "        lpar = params[str(l)]\n",
    "        outputs = np.dot(inputs, lpar['w']) + lpar['b']\n",
    "        inputs = nonlinearity(outputs)\n",
    "    return outputs\n",
    "\n",
    "# I'll allow separate scales for the weights and biases and refer to them\n",
    "# by names.  If you later add parameters to the dictionary, this will\n",
    "# still only add priors to the weights and biases.\n",
    "def log_gaussian_paragami(params, prior_par):\n",
    "    log_prior = 0.0\n",
    "    num_layers = len(params)\n",
    "    for l in range(num_layers):\n",
    "        lpar = params[str(l)]\n",
    "        log_prior += np.sum(norm.logpdf(lpar['w'], 0, prior_par['w_scale']))\n",
    "        log_prior += np.sum(norm.logpdf(lpar['b'], 0, prior_par['b_scale']))\n",
    "    return log_prior\n",
    "\n",
    "def logprob_paragami(params, inputs, targets, prior_par):\n",
    "    predictions = nn_predict_paragami(params, inputs)\n",
    "    return np.sum(norm.logpdf(predictions, targets, prior_par['noise_scale']))\n",
    "\n",
    "def folded_objective(params, prior_par):\n",
    "    return -logprob_paragami(params, inputs, targets, prior_par) \\\n",
    "           -log_gaussian_paragami(params, prior_par)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some functions for comparing the two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def assert_arrays_almost_equal(a, b, tol=1e-12):\n",
    "    if np.max(np.abs(a - b)) > tol:\n",
    "        raise ValueError('Not equal')\n",
    "\n",
    "def convert_params_to_paragami(params, nn_pattern):\n",
    "    params_paragami = dict()\n",
    "    for l in range(len(nn_pattern.keys())):\n",
    "        params_paragami[str(l)] = dict()\n",
    "        params_paragami[str(l)]['w'] = params[l][0]\n",
    "        params_paragami[str(l)]['b'] = params[l][1]\n",
    "    valid, msg = nn_pattern.validate_folded(params_paragami)\n",
    "    if not valid:\n",
    "        raise ValueError(msg)\n",
    "    return params_paragami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the initial parameters and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_scale = 0.1\n",
    "weight_prior_variance = 10.0\n",
    "layer_sizes= [1, 4, 4, 1]\n",
    "\n",
    "init_params = init_random_params(init_scale, layer_sizes=layer_sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the paragami patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict:\n",
      "\t[0] = OrderedDict:\n",
      "\t[w] = NumericArrayPattern (1, 4) (lb=-inf, ub=inf)\n",
      "\t[b] = NumericArrayPattern (4,) (lb=-inf, ub=inf)\n",
      "\t[1] = OrderedDict:\n",
      "\t[w] = NumericArrayPattern (4, 4) (lb=-inf, ub=inf)\n",
      "\t[b] = NumericArrayPattern (4,) (lb=-inf, ub=inf)\n",
      "\t[2] = OrderedDict:\n",
      "\t[w] = NumericArrayPattern (4, 1) (lb=-inf, ub=inf)\n",
      "\t[b] = NumericArrayPattern (1,) (lb=-inf, ub=inf)\n"
     ]
    }
   ],
   "source": [
    "nn_pattern = get_nn_pattern(layer_sizes)\n",
    "init_params_paragami = convert_params_to_paragami(init_params, nn_pattern)\n",
    "print(nn_pattern)\n",
    "\n",
    "prior_par = { 'w_scale': weight_prior_variance,\n",
    "              'b_scale': weight_prior_variance,\n",
    "              'noise_scale': 0.1}\n",
    "\n",
    "# A nice feature of paragami is that we can check whether parameters have valid\n",
    "# shapes and values.\n",
    "assert nn_pattern.validate_folded(init_params_paragami)[0]\n",
    "assert prior_pattern.validate_folded(prior_par)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For paragami, we need to define the flattened objective ourself.  \n",
    "\n",
    "For `autograd`, this is already done with the `@unflatten_optimizer` decorator on their implementation of `adam`.  Of course, one could define a similar decorator for `paragami`.  Personally, I like to be able to choose separately how I flatten and how I optimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init_params_flat = nn_pattern.flatten(init_params_paragami)\n",
    "prior_par_flat = prior_pattern.flatten(prior_par)\n",
    "flat_objective =  paragami.FlattenFunctionInput(\n",
    "    folded_objective, [ nn_pattern, prior_pattern ],\n",
    "    free=[True, False], argnums=[0, 1])\n",
    "\n",
    "def objective_paragami(par, t):\n",
    "    return flat_objective(par, prior_par_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Check that the two objectives are idential.\n",
    "assert np.abs(folded_objective(init_params_paragami, prior_par) -\n",
    "              objective(init_params, 0)) < 1e-12\n",
    "\n",
    "assert np.abs(objective_paragami(init_params_flat, 0) -\n",
    "              objective(init_params, 0)) < 1e-12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform optimization on the `autograd` version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing network parameters...\n",
      "Iteration 0 log likelihood -632.7841008314174\n",
      "Iteration 50 log likelihood -384.2830389865702\n",
      "Iteration 100 log likelihood -134.87342397542312\n",
      "Iteration 150 log likelihood -10.500006545543343\n",
      "Iteration 200 log likelihood -8.58780338607312\n",
      "Iteration 250 log likelihood -6.856004924106443\n",
      "Iteration 300 log likelihood -5.904673818790883\n",
      "Iteration 350 log likelihood -5.5546502379648075\n",
      "Iteration 400 log likelihood -5.430400278290989\n",
      "Iteration 450 log likelihood -5.465832705012744\n",
      "Iteration 500 log likelihood -5.345904727294112\n",
      "Iteration 550 log likelihood -5.3165477163621375\n",
      "Iteration 600 log likelihood -5.293316778869155\n",
      "Iteration 650 log likelihood -5.272816900458096\n",
      "Iteration 700 log likelihood -5.239799393394577\n",
      "Iteration 750 log likelihood -5.213782130592122\n",
      "Iteration 800 log likelihood -5.274903712577299\n",
      "Iteration 850 log likelihood -5.1663544276520526\n",
      "Iteration 900 log likelihood -5.1411319281978365\n",
      "Iteration 950 log likelihood -5.368775500962386\n",
      "Optimized in 3.2963759899139404 seconds.\n"
     ]
    }
   ],
   "source": [
    "def callback(params, t, g, print_every=50):\n",
    "    if t % print_every == 0:\n",
    "        print(\"Iteration {} log likelihood {}\".format(\n",
    "            t, -objective(params, t)))\n",
    "\n",
    "print(\"Optimizing network parameters...\")\n",
    "base_time = time.time()\n",
    "optimized_params = adam(grad(objective), init_params,\n",
    "                        step_size=0.01, num_iters=1000, callback=callback)\n",
    "base_time = time.time() - base_time\n",
    "print('Optimized in {} seconds.'.format(base_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform optimization the paragami version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing network parameters...\n",
      "Iteration 0 log likelihood -632.7841008314174\n",
      "Iteration 50 log likelihood -384.2830389865702\n",
      "Iteration 100 log likelihood -134.87342397542312\n",
      "Iteration 150 log likelihood -10.500006545543329\n",
      "Iteration 200 log likelihood -8.58780338607312\n",
      "Iteration 250 log likelihood -6.856004924106458\n",
      "Iteration 300 log likelihood -5.904673818790883\n",
      "Iteration 350 log likelihood -5.554650237964793\n",
      "Iteration 400 log likelihood -5.430400278291003\n",
      "Iteration 450 log likelihood -5.46583270501273\n",
      "Iteration 500 log likelihood -5.345904727294112\n",
      "Iteration 550 log likelihood -5.3165477163621375\n",
      "Iteration 600 log likelihood -5.293316778869155\n",
      "Iteration 650 log likelihood -5.2728169004581105\n",
      "Iteration 700 log likelihood -5.239799393394577\n",
      "Iteration 750 log likelihood -5.213782130592108\n",
      "Iteration 800 log likelihood -5.274903712577299\n",
      "Iteration 850 log likelihood -5.166354427652038\n",
      "Iteration 900 log likelihood -5.1411319281978365\n",
      "Iteration 950 log likelihood -5.368775500962386\n",
      "Optimized in 3.0171921253204346 seconds.\n"
     ]
    }
   ],
   "source": [
    "def callback_paragami(params_flat, t, g, print_every=50):\n",
    "    if t % print_every == 0:\n",
    "        print(\"Iteration {} log likelihood {}\".format(\n",
    "            t, -objective_paragami(params_flat, t)))\n",
    "\n",
    "print(\"Optimizing network parameters...\")\n",
    "paragami_time = time.time()\n",
    "optimized_params_flat = \\\n",
    "    adam(grad(objective_paragami), init_params_flat,\n",
    "         step_size=0.01, num_iters=1000, callback=callback_paragami)\n",
    "paragami_time = time.time() - paragami_time\n",
    "print('Optimized in {} seconds.'.format(paragami_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "autograd implementation optimized in 3.2963759899139404 seconds.\n",
      "paragami implementation optimized in 3.0171921253204346 seconds.\n"
     ]
    }
   ],
   "source": [
    "print('autograd implementation optimized in {} seconds.'.format(base_time))\n",
    "print('paragami implementation optimized in {} seconds.'.format(paragami_time))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
