{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from autograd import numpy as np\n",
    "import autograd\n",
    "from autograd.core import primitive, defvjp, defjvp\n",
    "from autograd.numpy.linalg import slogdet, solve, inv\n",
    "\n",
    "import scipy as sp\n",
    "import vittles\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: 0.0009748280108099906\n",
      "    hess: array([[2.44509221, 1.66677909, 1.72065082, 1.2325689 ],\n",
      "       [1.66677909, 3.00646763, 1.98422462, 1.30356959],\n",
      "       [1.72065082, 1.98422462, 3.22225725, 1.44557718],\n",
      "       [1.2325689 , 1.30356959, 1.44557718, 2.22147862]])\n",
      "     jac: array([-2.91433544e-16, -2.91867225e-16, -2.91867225e-16, -2.91867225e-16])\n",
      " message: 'A bad approximation caused failure to predict improvement.'\n",
      "    nfev: 4\n",
      "    nhev: 3\n",
      "     nit: 2\n",
      "    njev: 3\n",
      "  status: 2\n",
      " success: False\n",
      "       x: array([4.59090270e-04, 2.15422310e-04, 8.16495910e-05, 6.64732807e-04])\n",
      "Optimal parameter:  [4.59090270e-04 2.15422310e-04 8.16495910e-05 6.64732807e-04]\n"
     ]
    }
   ],
   "source": [
    "# Define an objective function.\n",
    "\n",
    "hyperdim = 3\n",
    "dim = 4\n",
    "a_mat = np.random.random((dim, dim)) \n",
    "a_mat = a_mat @ a_mat.T + np.eye(dim)\n",
    "def obj(par, hyperpar):\n",
    "    return \\\n",
    "        0.5 * np.dot(par, a_mat) @ par + \\\n",
    "        (np.mean(par - 0.5) ** 5) * (np.mean(hyperpar - 0.5) ** 5)\n",
    "\n",
    "hyperpar0 = np.zeros(hyperdim)\n",
    "\n",
    "get_obj_hess = autograd.hessian(obj, argnum=0)\n",
    "get_obj_grad_par = autograd.grad(obj, argnum=0)\n",
    "\n",
    "# Get the optimum.\n",
    "\n",
    "opt = sp.optimize.minimize(\n",
    "    lambda par: obj(par, hyperpar0),\n",
    "    np.zeros(dim),\n",
    "    method='trust-ncg',\n",
    "    jac=lambda par: get_obj_grad_par(par, hyperpar0),\n",
    "    hess=lambda par: get_obj_hess(par, hyperpar0),\n",
    "    tol=1e-16, callback=None, options={})\n",
    "par0 = opt.x\n",
    "\n",
    "# Specify a new hyperparameter.\n",
    "\n",
    "delta = 0.1 * np.random.random(hyperdim)\n",
    "hyperpar1 = hyperpar0 + delta\n",
    "obj(par0, hyperpar0)\n",
    "\n",
    "hess0 = get_obj_hess(par0, hyperpar0)\n",
    "get_obj_cross_hess = autograd.jacobian(get_obj_grad_par, argnum=1)\n",
    "cross_hess0 = get_obj_cross_hess(par0, hyperpar0)\n",
    "\n",
    "print(opt)\n",
    "print('Optimal parameter: ', par0)\n",
    "\n",
    "# Make sure we're at an optimum.\n",
    "assert(np.linalg.norm(get_obj_grad_par(par0, hyperpar0)) < 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def check_hyperpar(hyperpar):\n",
    "    if np.linalg.norm(hyperpar - hyperpar0) > 1e-8:\n",
    "        raise ValueError('Wrong value for hyperpar. ', hyperpar, ' != ', hyperpar0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with the linear approximation class, which uses reverse mode.\n",
    "obj_lin = vittles.HyperparameterSensitivityLinearApproximation(\n",
    "    obj,\n",
    "    par0,\n",
    "    hyperpar0,\n",
    "    validate_optimum=True,\n",
    "    factorize_hessian=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Should be zeros: [0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Using Martin's trick, higher-order derivatives of the optimum just take a few lines.\n",
    "\n",
    "@primitive\n",
    "def get_parhat(hyperpar):\n",
    "    check_hyperpar(hyperpar)\n",
    "    return par0\n",
    "\n",
    "print('Should be zeros:', get_parhat(hyperpar0) - par0)\n",
    "\n",
    "# Reverse mode\n",
    "def get_parhat_vjp(ans, hyperpar):\n",
    "    #check_hyperpar(hyperpar) # Need to find some way to do this with boxes    \n",
    "    def vjp(g):\n",
    "        return -1 * (get_obj_cross_hess(get_parhat(hyperpar), hyperpar).T @\n",
    "                     np.linalg.solve(get_obj_hess(get_parhat(hyperpar), hyperpar), g)).T\n",
    "    return vjp\n",
    "defvjp(get_parhat, get_parhat_vjp)\n",
    "\n",
    "# Forward mode\n",
    "def get_parhat_jvp(g, ans, hyperpar):\n",
    "    #check_hyperpar(hyperpar) # Need to find some way to do this with boxes    \n",
    "    return -1 * (np.linalg.solve(get_obj_hess(get_parhat(hyperpar), hyperpar),\n",
    "                                 get_obj_cross_hess(get_parhat(hyperpar), hyperpar)) @ g)\n",
    "defjvp(get_parhat, get_parhat_jvp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check reverse mode first derivatives against manual formula.\n",
    "get_dpar_dhyperpar = autograd.jacobian(get_parhat)\n",
    "dpar_dhyperpar = get_dpar_dhyperpar(hyperpar0)\n",
    "\n",
    "# Check that the first derivative matches.\n",
    "assert(np.linalg.norm(\n",
    "    dpar_dhyperpar -\n",
    "    obj_lin.get_dopt_dhyper()) < 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check forward mode first derivatives.\n",
    "\n",
    "# I prefer my interface to autograd's for forward mode.  Behind the scenes\n",
    "# it's the same thing.\n",
    "from vittles.sensitivity_lib import _append_jvp\n",
    "\n",
    "get_dpar_dhyperpar_delta = _append_jvp(get_parhat)\n",
    "dpar_dhyperpar_delta = get_dpar_dhyperpar_delta(hyperpar0, delta)\n",
    "\n",
    "# Check that the first derivative matches.\n",
    "assert(np.linalg.norm(\n",
    "    dpar_dhyperpar_delta -\n",
    "    obj_lin.get_dopt_dhyper() @ delta) < 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rgiordan/Documents/git_repos/vittles/vittles/sensitivity_lib.py:857: UserWarning: The ParametricSensitivityTaylorExpansion is experimental.\n",
      "  'The ParametricSensitivityTaylorExpansion is experimental.')\n"
     ]
    }
   ],
   "source": [
    "# Let's compare against the Taylor expansion class for higher derivatives.\n",
    "obj_taylor = vittles.ParametricSensitivityTaylorExpansion(\n",
    "    obj,\n",
    "    par0,\n",
    "    hyperpar0,\n",
    "    order=4)\n",
    "\n",
    "# Sanity check.\n",
    "assert(np.linalg.norm(\n",
    "    obj_taylor.evaluate_input_derivs(delta, max_order=1)[0] -\n",
    "    dpar_dhyperpar_delta) < 1e-8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second order\n",
      "Autograd forward mode time:\t 0.3968043327331543\n",
      "vittles time:\t\t\t 0.0041577816009521484\n"
     ]
    }
   ],
   "source": [
    "# Check that the second derivative reverse mode matches.\n",
    "autograd_time = time.time()\n",
    "get_d2par_dhyperpar2 = autograd.jacobian(get_dpar_dhyperpar)\n",
    "d2par_dhyperpar2 = get_d2par_dhyperpar2(hyperpar0)\n",
    "autograd_time = time.time() - autograd_time\n",
    "\n",
    "# Let's be generous and not count the einsum against autograd.\n",
    "d2par_dhyperpar2_delta2 = np.einsum('ijk,j,k->i', d2par_dhyperpar2, delta, delta)\n",
    "\n",
    "vittles_time = time.time()\n",
    "d2par_dhyperpar2_delta2_vittles = obj_taylor.evaluate_input_derivs(delta, max_order=2)[1]\n",
    "vittles_time = time.time() - vittles_time\n",
    "\n",
    "assert(np.linalg.norm(\n",
    "    d2par_dhyperpar2_delta2 -\n",
    "    d2par_dhyperpar2_delta2_vittles) < 1e-8)\n",
    "\n",
    "print('Second order')\n",
    "print('Autograd forward mode time:\\t', autograd_time)\n",
    "print('vittles time:\\t\\t\\t', vittles_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second order\n",
      "Autograd forward mode time:\t 0.039446353912353516\n",
      "vittles time:\t\t\t 0.0041577816009521484\n"
     ]
    }
   ],
   "source": [
    "# Check that forward mode second derivatives match.\n",
    "autograd_time = time.time()\n",
    "get_d2par_d2hyperpar_delta = _append_jvp(get_dpar_dhyperpar_delta)\n",
    "d2par_d2hyperpar_delta2 = get_d2par_d2hyperpar_delta(hyperpar0, delta, delta)\n",
    "autograd_time = time.time() - autograd_time\n",
    "\n",
    "assert(np.linalg.norm(\n",
    "    d2par_dhyperpar2_delta2 -\n",
    "    d2par_dhyperpar2_delta2_vittles) < 1e-8)\n",
    "\n",
    "print('Second order')\n",
    "print('Autograd forward mode time:\\t', autograd_time)\n",
    "print('vittles time:\\t\\t\\t', vittles_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the third derivative matches.\n",
    "\n",
    "## Reverse mode is super slow even in very low dimensions.\n",
    "# autograd_time = time.time()\n",
    "# get_d3par_dhyperpar3 = autograd.jacobian(get_d2par_dhyperpar2)\n",
    "# d3par_dhyperpar3 = get_d3par_dhyperpar3(hyperpar0)\n",
    "# autograd_time = time.time() - autograd_time\n",
    "# print('Autograd time:', autograd_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third order\n",
      "Autograd forward mode time:\t 0.09583282470703125\n",
      "vittles time:\t\t\t 0.017874479293823242\n"
     ]
    }
   ],
   "source": [
    "autograd_time = time.time()\n",
    "get_d3par_d3hyperpar_delta = _append_jvp(get_d2par_d2hyperpar_delta)\n",
    "d3par_dhyperpar3_delta3 = get_d3par_d3hyperpar_delta(hyperpar0, delta, delta, delta)\n",
    "autograd_time = time.time() - autograd_time\n",
    "\n",
    "vittles_time = time.time()\n",
    "d3par_dhyperpar3_delta3_vittles = obj_taylor.evaluate_input_derivs(delta, max_order=3)[2]\n",
    "vittles_time = time.time() - vittles_time\n",
    "\n",
    "assert(np.linalg.norm(\n",
    "    d3par_dhyperpar3_delta3 -\n",
    "    d3par_dhyperpar3_delta3_vittles) < 1e-8)\n",
    "\n",
    "print('Third order')\n",
    "print('Autograd forward mode time:\\t', autograd_time)\n",
    "print('vittles time:\\t\\t\\t', vittles_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fourth order\n",
      "Autograd forward mode time:\t 0.21596074104309082\n",
      "vittles time:\t\t\t 0.03277397155761719\n"
     ]
    }
   ],
   "source": [
    "# Fourth order.\n",
    "\n",
    "autograd_time = time.time()\n",
    "get_d4par_d4hyperpar_delta = _append_jvp(get_d3par_d3hyperpar_delta)\n",
    "d4par_dhyperpar4_delta4 = get_d4par_d4hyperpar_delta(hyperpar0, delta, delta, delta, delta)\n",
    "autograd_time = time.time() - autograd_time\n",
    "\n",
    "vittles_time = time.time()\n",
    "d4par_dhyperpar4_delta4_vittles = obj_taylor.evaluate_input_derivs(delta, max_order=4)[3]\n",
    "vittles_time = time.time() - vittles_time\n",
    "\n",
    "assert(np.linalg.norm(\n",
    "    d4par_dhyperpar4_delta4 -\n",
    "    d4par_dhyperpar4_delta4_vittles) < 1e-8)\n",
    "\n",
    "print('Fourth order')\n",
    "print('Autograd forward mode time:\\t', autograd_time)\n",
    "print('vittles time:\\t\\t\\t', vittles_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
