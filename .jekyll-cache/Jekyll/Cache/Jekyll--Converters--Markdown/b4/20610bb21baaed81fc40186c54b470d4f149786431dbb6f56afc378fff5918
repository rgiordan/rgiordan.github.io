I"ƒ<p>In the previous post, I introduced some notation and concepts that Iâ€™ll
now carry forward into an actual sketch of how to prove the BCLT.</p>

<h1 id="re-write-a-posterior-expectation-as-a-random-function-that-doesnt-diverge">Re-write a posterior expectation as a random function that doesnâ€™t diverge.</h1>

<p>Recall that, for the BCLT we need to analyze quantites of the following form:</p>

\[\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\theta) \exp(\sum_{n=1}^N \ell(x_n | \theta)) \pi(\theta) d\theta}
     {\int \exp(\sum_{n=1}^N \ell(x_n | \theta)) \pi(\theta) d\theta}.\]

<p>And recall that, to prove asymptotic normality of the MLE, we studied
the asymptotic behavior of the Taylor expansion of the
random log likelihood function,</p>

\[\theta \mapsto \frac{1}{N} \sum_{n=1}^N \ell(x_n \vert \theta).\]

<p>The factor of \(1/N\) in the MLE is a stark difference with the Bayesian
case, as the posterior involves the random function</p>

\[\theta \mapsto \sum_{n=1}^N \ell(x_n | \theta),\]

<p>which diverges as \(N \rightarrow \infty\)!  Before even beginning, we must
re-write our expectation in a form involving something that looks like sample
averages, not a divergent sum.</p>

<p>To do so, weâ€™ll first form a Taylor series expansion.  For the moment, weâ€™ll
expand around a generic \(\bar\theta\).</p>

\[\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta)(\theta - \bar\theta) +
    \frac{1}{2}\sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)(\theta - \bar\theta)^2 +
    \frac{1}{6}\sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)(\theta - \bar\theta)^3.\]

<p>At first glance, each term in the Taylor series still contains \(N\) terms,
so it might not seem like weâ€™ve gotten much.  However, we expect to be
able to choose \(\bar\theta\) so that \(|\theta - \bar\theta| = O(1 / \sqrt{N})\),
so that higher powers of \(\theta - \bar\theta\) will decrease asymptotically.</p>

<p>So letâ€™s try a parameterization that inflates \(\theta\) at the rate
\(\sqrt{N}\).  Specifically, for any \(\bar\theta\) depending only on the data,
we can define the Bayesian parameter</p>

\[\tau := \sqrt{N} (\theta - \bar\theta)
\quad\quad \Leftrightarrow \quad \quad
\theta = \frac{\tau}{\sqrt{N}} + \bar\theta.\]

<p>Plugging in, we have</p>

\[\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta) \tau +
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)\tau^2 +
    \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3.\]

<p>Note that in this new formula, the derivatives are still with respect to
\(\theta\) â€” weâ€™ve simply plugged \(\tau\) into the previous expression.</p>

<p>Now weâ€™re getting somewhere, because the quadratic term looks like what we
expect from a normal distribution, and the cubic term is a sample average
times an additional power of \(1/\sqrt{N}\), and so goes to zero.</p>

<p>Two terms need to be dealt with.  First, the term \(\frac{1}{\sqrt{N}}
\sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta) \tau\) is \(O_p(1)\) by an ordinary
central limit theorem.  Fortunately, we can get rid of this term by simply
evaluating at \(\bar\theta = \hat\theta\), since then \(\sum_{n=1}^N
\ell_{(1)}(x_n | \bar\theta)\) is identically zero by the first-order condition
defining \(\hat\theta\). And, second, we observe that \(\sum_{n=1}^N \ell(x_n |
\bar\theta)\) does not depend on \(\theta\), and so cancels in the numerator and
denominator of \(\mathbb{E}[\phi(\theta) | x]\).</p>

<p>Before we plug into \(\mathbb{E}[\phi(\theta) | x]\), we need to deal with the
effect of the change of variables on the prior.  By the ordinary rules of
density transformation, \(\pi(\theta) d\theta\) = \(\frac{1}{\sqrt{N} }\pi(\tau /
\sqrt{N} + \hat\theta) d\tau\).  Again, the factor of \(\frac{1}{\sqrt{N} }\)
will cancel in the numerator and denominator.  Putting all together, we get</p>

\[\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\tau / \sqrt{N} + \hat\theta) \exp\left(
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)\tau^2 +
    \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3
\right)
    \pi(\tau / \sqrt{N} + \hat\theta) d\tau}
 {\int \exp\left(
     \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)\tau^2 +
     \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3
  \right)
  \pi(\tau / \sqrt{N} + \hat\theta) d\tau}.\]

<p>We can now study \(\mathbb{E}[\phi(\theta) | x]\) by studying integrals
of the following form, depending on some generic function \(\psi(\theta)\):</p>

\[I(\psi) :=
    \int \psi(\tau / \sqrt{N} + \hat\theta) \exp\left(
        \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \hat\theta)\tau^2 +
        \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3
    \right)
        \pi(\tau / \sqrt{N} + \hat\theta) d\tau,\]

<p>since \(\mathbb{E}[\phi(\theta) | x] = I(\phi) / I(1)\).  Importantly,
\(I(\psi)\) is an integral over quantities that will not diverge as \(N
\rightarrow \infty\), and we have accomplished the goal of this first section.</p>

<h1 id="divide-the-domain-of-integration-into-regions">Divide the domain of integration into regions.</h1>

<p>As we mentioned in the last post, a key difficulty with Bayesian CLTs compared
to the behavior of the MLE is that the integral \(I(\psi)\) still depends on the
entire domain of \(\theta\) (or, equivalently, \(\tau\)) through the integral.
Letâ€™s now think about what sorts of problems we might have with \(I(\psi)\) for
\(\tau\), or equivalently \(\theta\), in different regions of the real line.</p>

<p>First, the easy region.  Suppose we have a shrinking ball centered at
\(\hat\theta\) called region 1, or \(R_1\).  For example, we might take
some real number $\delta_1 &gt; 0$ and define</p>

\[R_1 = \{ \theta: |\theta - \hat\theta| &lt; \delta_1 / \log N \}.\]
:ET