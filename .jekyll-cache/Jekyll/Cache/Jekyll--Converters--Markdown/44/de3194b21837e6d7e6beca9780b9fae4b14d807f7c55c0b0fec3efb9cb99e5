I"^<p>In the previous post, I introduced some notation and concepts that Iâ€™ll
now carry forward into an actual sketch of how to prove the BCLT.</p>

<h1 id="re-write-a-posterior-expectation-as-a-random-function-that-doesnt-diverge">Re-write a posterior expectation as a random function that doesnâ€™t diverge.</h1>

<p>Recall that, for the BCLT we need to analyze quantites of the following form:</p>

\[\mathbb{E}[\phi(\theta) | x] =
\frac{\int \phi(\theta) \exp(\sum_{n=1}^N \ell(x_n | \theta))p(\theta) d\theta}
     {\int \exp(\sum_{n=1}^N \ell(x_n | \theta))p(\theta) d\theta}.\]

<p>And recall that, to prove asymptotic normality of the MLE, we studied
the asymptotic behavior of the Taylor expansion of the
random log likelihood function,</p>

\[\theta \mapsto \frac{1}{N} \sum_{n=1}^N \ell(x_n \vert \theta).\]

<p>The factor of \(1/N\) in the MLE is a stark difference with the Bayesian
case, as the posterior involves the random function</p>

\[\theta \mapsto \sum_{n=1}^N \ell(x_n | \theta),\]

<p>which diverges as \(N \rightarrow \infty\)!  Before even beginning, we must
re-write our expectation in a form involving something that looks like sample
averages, not a divergent sum.</p>

<p>To do so, weâ€™ll first form a Taylor series expansion.  For the moment, weâ€™ll
expand around a generic \(\bar\theta\).</p>

\[\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta)(\theta - \bar\theta) +
    \frac{1}{2}\sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)(\theta - \bar\theta)^2 +
    \frac{1}{6}\sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)(\theta - \bar\theta)^3.\]

<p>At first glance, each term in the Taylor series still contains \(N\) terms,
so it might not seem like weâ€™ve gotten much.  However, we expect to be
able to choose \(\bar\theta\) so that \(|\theta - \bar\theta| = O(1 / \sqrt{N})\),
so that higher powers of \(\theta - \bar\theta\) will decrease asymptotically.</p>

<p>So letâ€™s try a parameterization that inflates \(\theta\) at the rate
\(\sqrt{N}\).  Specifically, for any \(\bar\theta\) depending only on the data,
we can define the Bayesian parameter</p>

\[\tau := \sqrt{N} (\theta - \bar\theta)
\quad\quad \Leftrightarrow \quad \quad
\theta = \frac{\tau}{\sqrt{N}} + \bar\theta.\]

<p>(Under this change of variables, the prior will transform so that \(p(\tau) d\tau = p(\theta) d\theta\),
but we will deal with that later.)  Plugging in, we have</p>

\[\sum_{n=1}^N \ell(x_n | \theta) =
    \sum_{n=1}^N \ell(x_n | \bar\theta) +
    \frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta) \tau +
    \frac{1}{2} \frac{1}{N} \sum_{n=1}^N \ell_{(2)}(x_n | \bar\theta)\tau^2 +
    \frac{1}{6} \frac{1}{N^{3/2}} \sum_{n=1}^N \ell_{(3)}(x_n | \tilde\theta)\tau^3.\]

<p>Note that in this new formula, the derivatives are still with respect to
\(\theta\) â€” weâ€™ve simply plugged \(\tau\) into the previous expression.</p>

<p>Now weâ€™re getting somewhere, because the quadratic term looks like what we
expect from a normal distribution, and the cubic term is a sample average
times an additional power of \(1/\sqrt{N}\), and so goes to zero.</p>

<p>However, the term \(\frac{1}{\sqrt{N}} \sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta)
\tau\) is \(O_p(1)\) by an ordinary central limit theorem.  Fortunately, we can
get rid of this term by simply evaluating at \(\bar\theta = \hat\theta\), since
then \(\sum_{n=1}^N \ell_{(1)}(x_n | \bar\theta)\) is identically zero by the
first-order condition defining \(\hat\theta\).</p>

<table>
  <tbody>
    <tr>
      <td>Finally, we observe that $$\sum_{n=1}^N \ell(x_n</td>
      <td>\bar\theta)$$</td>
    </tr>
  </tbody>
</table>
:ET